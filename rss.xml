<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Filtered Computational Linguistics Papers</title><link>https://github.com/hre06/arxiv_rss/rss.xml</link><description>Filtered arXiv papers on computational linguistics</description><lastBuildDate>Mon, 14 Oct 2024 23:28:12 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference</title><link>https://arxiv.org/abs/2410.08289</link><description>arXiv:2410.08289v1 Announce Type: new 
Abstract: As the cultural heritage sector increasingly adopts technologies like Retrieval-Augmented Generation (RAG) to provide more personalised search experiences and enable conversations with collections data, the demand for specialised evaluation datasets has grown. While end-to-end system testing is essential, it's equally important to assess individual components. We target the final, answering task, which is well-suited to Machine Reading Comprehension (MRC). Although existing MRC datasets address general domains, they lack the specificity needed for cultural heritage information. Unfortunately, the manual creation of such datasets is prohibitively expensive for most heritage institutions. This paper presents a cost-effective approach for generating domain-specific MRC datasets with increased difficulty using Reinforcement Learning from Human Feedback (RLHF) from synthetic preference data. Our method leverages the performance of existing question-answering models on a subset of SQuAD to create a difficulty metric, assuming that more challenging questions are answered correctly less frequently. This research contributes: (1) A methodology for increasing question difficulty using PPO and synthetic data; (2) Empirical evidence of the method's effectiveness, including human evaluation; (3) An in-depth error analysis and study of emergent phenomena; and (4) An open-source codebase and set of three llama-2-chat adapters for reproducibility and adaptation.</description><pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate></item><item><title>MELO: An Evaluation Benchmark for Multilingual Entity Linking of Occupations</title><link>https://arxiv.org/abs/2410.08319</link><description>arXiv:2410.08319v1 Announce Type: new 
Abstract: We present the Multilingual Entity Linking of Occupations (MELO) Benchmark, a new collection of 48 datasets for evaluating the linking of entity mentions in 21 languages to the ESCO Occupations multilingual taxonomy. MELO was built using high-quality, pre-existent human annotations. We conduct experiments with simple lexical models and general-purpose sentence encoders, evaluated as bi-encoders in a zero-shot setup, to establish baselines for future research. The datasets and source code for standardized evaluation are publicly available at https://github.com/Avature/melo-benchmark</description><pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate></item><item><title>The language of sound search: Examining User Queries in Audio Search Engines</title><link>https://arxiv.org/abs/2410.08324</link><description>arXiv:2410.08324v1 Announce Type: new 
Abstract: This study examines textual, user-written search queries within the context of sound search engines, encompassing various applications such as foley, sound effects, and general audio retrieval. Current research inadequately addresses real-world user needs and behaviours in designing text-based audio retrieval systems. To bridge this gap, we analysed search queries from two sources: a custom survey and Freesound website query logs. The survey was designed to collect queries for an unrestricted, hypothetical sound search engine, resulting in a dataset that captures user intentions without the constraints of existing systems. This dataset is also made available for sharing with the research community. In contrast, the Freesound query logs encompass approximately 9 million search requests, providing a comprehensive view of real-world usage patterns. Our findings indicate that survey queries are generally longer than Freesound queries, suggesting users prefer detailed queries when not limited by system constraints. Both datasets predominantly feature keyword-based queries, with few survey participants using full sentences. Key factors influencing survey queries include the primary sound source, intended usage, perceived location, and the number of sound sources. These insights are crucial for developing user-centred, effective text-based audio retrieval systems, enhancing our understanding of user behaviour in sound search contexts.</description><pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate></item></channel></rss>